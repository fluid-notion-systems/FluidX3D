# Multi-GPU Surface Export Research

## Executive Summary

The marching cubes algorithm for surface export in FluidX3D faces significant challenges when operating across multiple GPUs due to domain decomposition and boundary handling. This document analyzes these challenges and proposes solutions for robust multi-GPU surface export. The simplest and most practical solution is to export separate surface meshes from each GPU, avoiding boundary complications entirely.

## FluidX3D Multi-GPU Architecture

### Domain Decomposition

FluidX3D uses spatial domain decomposition to distribute the simulation across multiple GPUs:

```
Total domain: Nx × Ny × Nz
Split into: Dx × Dy × Dz domains
Each GPU handles: (Nx/Dx) × (Ny/Dy) × (Nz/Dz) cells
```

### Halo Regions

Each domain maintains a 1-cell thick halo (ghost) region at its boundaries:

```cl
bool is_halo(const uxx n) {
  const uint3 xyz = coordinates(n);
  return ((def_Dx > 1u) & (xyz.x == 0u || xyz.x >= def_Nx - 1u)) ||
         ((def_Dy > 1u) & (xyz.y == 0u || xyz.y >= def_Ny - 1u)) ||
         ((def_Dz > 1u) & (xyz.z == 0u || xyz.z >= def_Nz - 1u));
}
```

**Key Points:**
- Halo cells contain copies of data from neighboring domains
- Updated through PCIe transfers between GPUs
- Essential for stencil operations that need neighbor data

### Communication System

FluidX3D implements a sophisticated field communication system:

1. **Extract Phase**: Copy boundary data to transfer buffers
2. **PCIe Transfer**: Move data between GPU memories
3. **Insert Phase**: Update halo regions with received data

Fields that are communicated:
- `fi` - Distribution functions
- `rho_u_flags` - Density, velocity, and cell flags
- `phi_massex_flags` - Level set function, mass excess, flags
- `T` - Temperature
- `gi` - Temperature distribution functions

## Challenges for Multi-GPU Surface Export

### 1. Marching Cubes Across Domain Boundaries

The marching cubes algorithm requires an 8-cell cube (2×2×2) to generate triangles:

```
   4 -------- 5
  /|         /|
 / |        / |
7 -------- 6  |
|  0 ------|--1
| /        | /
|/         |/
3 -------- 2
```

**Problem**: At domain boundaries, half of the cube vertices belong to the neighboring domain's halo region.

**Consequences**:
- Triangles at boundaries may be:
  - Duplicated (generated by both domains)
  - Missing (neither domain generates them)
  - Inconsistent (different phi values due to communication lag)

### 2. Phi Field Synchronization

The phi field (level set function) must be synchronized across domains for consistent surface extraction:

- Current system updates phi through `communicate_phi_massex_flags()`
- This happens during the surface reconstruction phase
- Timing of updates is critical for consistent marching cubes

### 3. Triangle Counting Inconsistencies

When counting triangles for memory allocation:

```cl
kernel void count_surface_triangles(const global float *phi, global uint *triangle_count) {
    // Each domain counts its own triangles
    // But boundary triangles may be counted multiple times
}
```

**Issues**:
- Atomic operations only work within a single GPU
- No global atomic counter across all GPUs
- Over-allocation if boundaries are double-counted

### 4. Global Coordinate System

Each domain works in local coordinates:
- Local: `(0, 0, 0)` to `(Nx/Dx-1, Ny/Dy-1, Nz/Dz-1)`
- Global: Requires offset based on domain position

Surface vertices need transformation:
```cl
const float3 global_offset = (float3)(
    domain_x * (Nx/Dx),
    domain_y * (Ny/Dy),
    domain_z * (Nz/Dz)
);
```

### 5. Memory Management Across GPUs

Current implementation assumes single GPU:
- `surface_vertices` allocated on one device
- No mechanism to gather triangles from all domains
- Buffer overflow protection only works locally

## Detailed Analysis of Boundary Cases

### Case 1: Face Boundaries (6 faces per domain)

Cubes straddling face boundaries have 4 vertices in each domain:
```
Domain A: vertices 0,1,2,3
Domain B: vertices 4,5,6,7 (in halo of A)
```

### Case 2: Edge Boundaries (12 edges per domain)

Cubes at edge intersections involve 3-4 domains:
```
Domain A: vertices 0,1
Domain B: vertices 2,3 (in halo of A)
Domain C: vertices 4,5 (in halo of A)
Domain D: vertices 6,7 (in halos of A,B,C)
```

### Case 3: Corner Boundaries (8 corners per domain)

Cubes at corners involve up to 8 domains in 3D.

## Proposed Solutions

### Solution 1: Boundary Ownership Protocol

Establish clear ownership rules for boundary cubes:

```cl
bool owns_boundary_cube(uint3 local_pos, uint3 domain_pos) {
    // Domain with lower index owns shared boundaries
    bool owns_x = (local_pos.x == Nx/Dx-1) ? false : true;
    bool owns_y = (local_pos.y == Ny/Dy-1) ? false : true;
    bool owns_z = (local_pos.z == Nz/Dz-1) ? false : true;
    
    // Special handling for domain edges
    if (domain_pos.x == Dx-1) owns_x = true;
    if (domain_pos.y == Dy-1) owns_y = true;
    if (domain_pos.z == Dz-1) owns_z = true;
    
    return owns_x && owns_y && owns_z;
}
```

### Solution 2: Extended Halo for Surface Export

Implement a specialized 2-cell halo for marching cubes:

```cl
bool is_halo_surface(const uint3 xyz) {
    return ((def_Dx > 1u) & (xyz.x <= 1u || xyz.x >= def_Nx - 2u)) ||
           ((def_Dy > 1u) & (xyz.y <= 1u || xyz.y >= def_Ny - 2u)) ||
           ((def_Dz > 1u) & (xyz.z <= 1u || xyz.z >= def_Nz - 2u));
}
```

This ensures all cube vertices are available locally.

### Solution 3: Two-Phase Triangle Generation

**Phase 1: Count triangles with ownership**
```cl
kernel void count_surface_triangles_owned(
    const global float *phi,
    global uint *triangle_count,
    const uint3 domain_offset,
    const uint3 domain_id
) {
    // Only count triangles this domain owns
    if (!owns_boundary_cube(local_pos, domain_id)) return;
    // ... marching cubes counting
}
```

**Phase 2: Generate triangles with global coordinates**
```cl
kernel void export_surface_owned(
    const global float *phi,
    global float *vertices,
    global uint *triangle_count,
    const uint3 domain_offset,
    const uint3 domain_id
) {
    // Only generate triangles this domain owns
    // Add domain_offset to vertex positions
}
```

### Solution 4: Distributed Surface Assembly

Implement a gather operation for the final surface:

```cpp
void LBM::gather_surface_from_domains() {
    // 1. Each domain exports its owned triangles
    for(uint d=0; d<get_D(); d++) {
        lbm_domain[d]->enqueue_export_surface();
    }
    
    // 2. Get triangle counts from each domain
    vector<ulong> domain_triangle_counts(get_D());
    for(uint d=0; d<get_D(); d++) {
        domain_triangle_counts[d] = lbm_domain[d]->get_triangle_count();
    }
    
    // 3. Allocate unified buffer
    ulong total_triangles = accumulate(
        domain_triangle_counts.begin(),
        domain_triangle_counts.end(),
        0ull
    );
    
    // 4. Copy triangles with proper offsets
    ulong offset = 0;
    for(uint d=0; d<get_D(); d++) {
        copy_triangles_from_domain(d, offset);
        offset += domain_triangle_counts[d];
    }
}
```

### Solution 5: Consistency Guarantees

Ensure phi field consistency before surface export:

```cpp
void LBM::prepare_surface_export() {
    // Synchronize all fields
    communicate_phi_massex_flags();
    for(uint d=0; d<get_D(); d++) {
        lbm_domain[d]->finish_queue();
    }
    
    // Double-buffer phi for consistency
    for(uint d=0; d<get_D(); d++) {
        lbm_domain[d]->snapshot_phi_field();
    }
}
```

## Implementation Recommendations

### 1. Recommended Solution: Per-GPU Surface Export

The simplest and most robust approach is to export separate surface meshes from each GPU domain:

```cpp
void LBM::export_surface_per_gpu() {
    // Each domain exports its own surface independently
    for(uint d=0u; d<get_D(); d++) {
        lbm_domain[d]->enqueue_export_surface();
        
        // Get vertices from this domain
        float* vertices = lbm_domain[d]->get_surface_vertices();
        ulong triangle_count = lbm_domain[d]->get_triangle_count();
        
        // Save to separate files
        string filename = "surface_domain_" + to_string(d) + ".stl";
        write_stl_file(filename, vertices, triangle_count, 
                      lbm_domain[d]->get_domain_offset());
    }
}
```

#### Advantages:
- **No boundary issues**: Each domain handles only its local cells
- **Simple implementation**: No ownership protocols or extended halos needed
- **Parallel export**: Each GPU can export simultaneously
- **No communication overhead**: No need to gather triangles
- **Easier debugging**: Can inspect individual domain meshes

#### Implementation Details:

**1. Modify Export to Include Domain Offset:**
```cl
kernel void export_surface(const global float *phi, global float *vertices,
                          global uint *triangle_count, const ulong max_triangles,
                          const uint3 domain_offset) {
    // ... existing marching cubes code ...
    
    // Add domain offset to vertex positions
    const float3 global_offset = (float3)(
        (float)domain_offset.x,
        (float)domain_offset.y,
        (float)domain_offset.z
    );
    
    for(uint i = 0u; i < tn; i++) {
        const float3 p0 = triangles[3u*i] + offset + global_offset;
        const float3 p1 = triangles[3u*i+1u] + offset + global_offset;
        const float3 p2 = triangles[3u*i+2u] + offset + global_offset;
        // ... write vertices ...
    }
}
```

**2. STL Writer with Domain Information:**
```cpp
void write_stl_file(const string& filename, float* vertices, 
                    ulong triangle_count, uint3 domain_offset) {
    ofstream file(filename, ios::binary);
    
    // STL header with domain info
    char header[80] = {0};
    sprintf(header, "FluidX3D surface domain at offset (%u,%u,%u)",
            domain_offset.x, domain_offset.y, domain_offset.z);
    file.write(header, 80);
    
    // Write triangles...
}
```

**3. Post-Processing Merge Script:**
```python
# merge_surfaces.py
import numpy as np
import trimesh

def merge_domain_surfaces(num_domains):
    meshes = []
    for i in range(num_domains):
        mesh = trimesh.load(f'surface_domain_{i}.stl')
        meshes.append(mesh)
    
    # Combine all meshes
    combined = trimesh.util.concatenate(meshes)
    
    # Optional: remove duplicate vertices at boundaries
    combined.merge_vertices()
    
    # Save merged result
    combined.export('surface_merged.stl')
```

#### Handling Boundary Gaps:

Since each domain only generates triangles for cells it fully contains, there may be small gaps at domain boundaries. These can be addressed:

1. **Accept small gaps**: For visualization, tiny gaps are often invisible
2. **Overlap domains slightly**: Generate triangles for boundary cells in both domains
3. **Post-process merge**: Use mesh repair tools to close gaps

### 2. Alternative Solution (Single-GPU Fallback)

For immediate functionality, implement single-GPU surface export:

```cpp
void LBM::enqueue_export_surface() {
    if(get_D() > 1) {
        print_warning("Surface export currently supports single-GPU only. "
                     "Exporting from domain 0.");
        lbm_domain[0]->enqueue_export_surface();
    } else {
        lbm_domain[0]->enqueue_export_surface();
    }
}
```

### 2. Short-term Solution (Boundary Ownership)

Implement the boundary ownership protocol:
- Modify counting kernel to respect ownership
- Modify export kernel to respect ownership
- Add domain offset to vertex coordinates

### 4. Complex Solution (Extended Halo)

Implement 2-cell halo regions for surface operations:
- Extend halo communication for phi field
- Modify marching cubes to work with extended halos
- Optimize communication patterns

## Testing Strategy for Per-GPU Export

### Test Case 1: Visual Inspection
- Export surfaces from 2×2×2 domain setup
- Load all 8 STL files in Blender/ParaView
- Verify alignment and check for gaps

### Test Case 2: Automated Validation
```python
def validate_domain_exports(domain_files):
    for i, file in enumerate(domain_files):
        mesh = trimesh.load(file)
        
        # Check vertices are within expected bounds
        domain_x = i % Dx
        domain_y = (i // Dx) % Dy
        domain_z = i // (Dx * Dy)
        
        min_bound = [domain_x * (Nx/Dx), 
                     domain_y * (Ny/Dy), 
                     domain_z * (Nz/Dz)]
        max_bound = [(domain_x+1) * (Nx/Dx), 
                     (domain_y+1) * (Ny/Dy), 
                     (domain_z+1) * (Nz/Dz)]
        
        assert np.all(mesh.vertices >= min_bound)
        assert np.all(mesh.vertices <= max_bound)
```

### Test Case 3: Performance Comparison
- Measure export time for per-GPU vs gathered approach
- Compare file I/O overhead vs communication overhead

## Original Testing Strategy

### Test Case 1: Simple Sphere
- Place sphere center at domain boundary
- Verify no duplicate/missing triangles
- Check watertight property

### Test Case 2: Thin Sheet
- Create fluid sheet crossing multiple domains
- Verify continuous surface across boundaries

### Test Case 3: Complex Geometry
- Dam break across 2×2×2 domains
- Compare with single-GPU reference

### Validation Metrics
1. Triangle count consistency
2. Vertex position continuity
3. Surface area conservation
4. Visual inspection in external tools

## Performance Considerations

### Communication Overhead
- Additional phi synchronization: ~1-2% overhead
- Extended halo (if implemented): ~5-10% overhead
- Triangle gathering: Negligible for typical exports

### Memory Requirements
- Per-domain buffers: Same as single-GPU
- Gather buffer: Sum of all domain triangles
- Extended halo: +12.5% memory for phi field

## Conclusion

Multi-GPU surface export in FluidX3D requires careful handling of domain boundaries to avoid artifacts. However, the simplest and most practical solution is to export separate surface meshes from each GPU domain. The recommended implementation strategy is:

1. **Immediate**: Implement per-GPU surface export
   - Each domain exports its own STL file
   - No boundary handling complications
   - Post-process merge if single mesh needed

2. **Optional Enhancement**: Add boundary overlap
   - Export boundary cells from multiple domains
   - Reduces gaps at boundaries
   - Slight increase in triangle count

3. **Future Complex Solution**: Unified export with boundary protocols
   - Only if seamless single mesh is critical
   - Significantly more complex implementation

The per-GPU export approach provides immediate functionality with minimal code changes and is suitable for most use cases. Users can easily merge the resulting meshes using standard 3D tools or scripts when a unified surface is needed.